Auks API request failed : krb5 cred : unable to read credential cache
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:42915'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:46237'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:37939'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:36269'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:44127'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:35581'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:36751'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:46767'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:43919'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:44513'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:40935'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:40087'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:42997'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:46331'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:35477'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:42195'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:37935'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:44051'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:37607'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:45909'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:46179'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:44967'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:32799'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:42493'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:41007'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:32859'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:46595'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:44721'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:38345'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:45339'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:36695'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.19:45289'
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:44851
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:44851
distributed.worker - INFO -          dashboard at:           10.32.2.19:37695
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-pe2e7y_v
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:43303
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:32847
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:35773
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:43303
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:44319
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:32847
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:44267
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:35773
distributed.worker - INFO -          dashboard at:           10.32.2.19:36981
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:36089
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:46553
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:44319
distributed.worker - INFO -          dashboard at:           10.32.2.19:34261
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:44267
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO -          dashboard at:           10.32.2.19:33507
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:36089
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:46553
distributed.worker - INFO -          dashboard at:           10.32.2.19:40215
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:           10.32.2.19:42533
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:33221
distributed.worker - INFO -          dashboard at:           10.32.2.19:37467
distributed.worker - INFO -          dashboard at:           10.32.2.19:42015
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:33481
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:33221
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:33065
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:46653
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:33481
distributed.worker - INFO -          dashboard at:           10.32.2.19:36709
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:45753
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:33065
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:46653
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:45753
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -          dashboard at:           10.32.2.19:39747
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:38945
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-jaw0mbe7
distributed.worker - INFO -          dashboard at:           10.32.2.19:38207
distributed.worker - INFO -          dashboard at:           10.32.2.19:33629
distributed.worker - INFO -          dashboard at:           10.32.2.19:36267
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-a78qjck0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:44691
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-uixsrh64
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:38945
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-m5_vdkgh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:44691
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-kz1gmq5g
distributed.worker - INFO -          dashboard at:           10.32.2.19:32929
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-br_tk8pe
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-jd49t6rn
distributed.worker - INFO -          dashboard at:           10.32.2.19:39827
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-ydtj36e1
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:40793
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-y67jwj__
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:35913
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-aubccuhs
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:40793
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-vhy0qkli
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:35913
distributed.worker - INFO -          dashboard at:           10.32.2.19:41215
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-vw38hw6d
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-3k2auw7b
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:           10.32.2.19:40179
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-ywoj9ci3
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:37061
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:37061
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:           10.32.2.19:39917
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-58nvwlnv
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-sblnn0xd
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:36703
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:36703
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -          dashboard at:           10.32.2.19:43885
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-yy799hbc
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:40277
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:40277
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO -          dashboard at:           10.32.2.19:42611
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:45537
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:45537
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:38795
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO -          dashboard at:           10.32.2.19:43947
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:35097
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:40489
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:38795
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-pi7emzwb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:35097
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:40489
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO -          dashboard at:           10.32.2.19:42939
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:           10.32.2.19:44781
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:           10.32.2.19:39399
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-anbfach6
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-lf32kx6k
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:35083
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:42691
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-zvwb62ca
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:35083
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:42691
distributed.worker - INFO -          dashboard at:           10.32.2.19:38593
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:40545
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-hu0pef58
distributed.worker - INFO -          dashboard at:           10.32.2.19:40891
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-3rjmu0mv
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:40545
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:           10.32.2.19:44153
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-xjj0lwmu
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-pmd97opa
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-ahuvsjh_
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:35599
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:35599
distributed.worker - INFO -          dashboard at:           10.32.2.19:38163
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:41083
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:34543
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:34543
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:           10.32.2.19:37989
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:41083
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-i_ncj70u
distributed.worker - INFO -          dashboard at:           10.32.2.19:34707
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-kb5amb88
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-fxlf8tta
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:40937
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:40937
distributed.worker - INFO -          dashboard at:           10.32.2.19:35373
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-o4jkrhr1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.19:33933
distributed.worker - INFO -          Listening to:     tcp://10.32.2.19:33933
distributed.worker - INFO -          dashboard at:           10.32.2.19:35055
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-md5r0jtv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:42727
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:42915'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:46237'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:36269'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:44267
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:37939'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:32847
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:43919'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:40937
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:44127'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:40277
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:35581'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:34543
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:36751'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:44851
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:46767'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:40545
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:44513'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:45537
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:40935'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:40087'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:37061
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:38945
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:42997'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:46331'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:36703
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:35477'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:36089
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:42195'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:35097
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:33065
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:37935'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:42691
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:44051'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:33481
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:37607'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:45909'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:44691
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:46179'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:44967'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:46553
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:33221
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:45753
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:32799'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:42493'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:44319
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:40793
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:41007'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:32859'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:35773
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:35083
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:46595'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:44721'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:45339'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:38795
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:38345'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:41083
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:46653
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:40489
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:36695'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:33933
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.19:45289'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:35599
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:43303
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:35913
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:42727
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:42727
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:42727
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:42727
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:42727
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:42727
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:34543
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:44691
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:44319
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:38795
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x7fd65a992cd0>>, <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py:1045> exception=CommClosedError('ConnectionPool not running. Status: Status.closed')>)
Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/core.py", line 250, in connect
    async def connect(
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 1043, in connect
    comm = await fut
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py", line 1058, in handle_scheduler
    await self.close(report=False)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py", line 1275, in close
    await r.close_gracefully()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 871, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 1054, in connect
    raise CommClosedError(
distributed.comm.core.CommClosedError: ConnectionPool not running. Status: Status.closed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x7fc62de66cd0>>, <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py:1045> exception=CommClosedError('ConnectionPool not running. Status: Status.closed')>)
Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/core.py", line 250, in connect
    async def connect(
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 1043, in connect
    comm = await fut
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py", line 1058, in handle_scheduler
    await self.close(report=False)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py", line 1275, in close
    await r.close_gracefully()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 871, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 1054, in connect
    raise CommClosedError(
distributed.comm.core.CommClosedError: ConnectionPool not running. Status: Status.closed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x7f0129bd6cd0>>, <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py:1045> exception=CommClosedError('ConnectionPool not running. Status: Status.closed')>)
Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/core.py", line 250, in connect
    async def connect(
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 1043, in connect
    comm = await fut
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py", line 1058, in handle_scheduler
    await self.close(report=False)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py", line 1275, in close
    await r.close_gracefully()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 871, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 1054, in connect
    raise CommClosedError(
distributed.comm.core.CommClosedError: ConnectionPool not running. Status: Status.closed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x7f3ae444fcd0>>, <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py:1045> exception=CommClosedError('ConnectionPool not running. Status: Status.closed')>)
Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/core.py", line 250, in connect
    async def connect(
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 1043, in connect
    comm = await fut
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py", line 1058, in handle_scheduler
    await self.close(report=False)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py", line 1275, in close
    await r.close_gracefully()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 871, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 1054, in connect
    raise CommClosedError(
distributed.comm.core.CommClosedError: ConnectionPool not running. Status: Status.closed
distributed.worker - INFO - Stopping worker at tcp://10.32.2.19:35913
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x7fa23f596cd0>>, <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py:1045> exception=CommClosedError('ConnectionPool not running. Status: Status.closed')>)
Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/core.py", line 250, in connect
    async def connect(
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 1043, in connect
    comm = await fut
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/tornado/ioloop.py", line 765, in _discard_future_result
    future.result()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py", line 1058, in handle_scheduler
    await self.close(report=False)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py", line 1275, in close
    await r.close_gracefully()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 871, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 1054, in connect
    raise CommClosedError(
distributed.comm.core.CommClosedError: ConnectionPool not running. Status: Status.closed
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - INFO - Worker process 284761 was killed by signal 15
distributed.nanny - INFO - Worker process 284807 was killed by signal 15
distributed.nanny - INFO - Worker process 284786 was killed by signal 15
distributed.nanny - INFO - Worker process 284804 was killed by signal 15
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/nanny.py", line 335, in start
    response = await self.instantiate()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/nanny.py", line 404, in instantiate
    result = await asyncio.wait_for(
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/asyncio/tasks.py", line 468, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 273, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/asyncio/tasks.py", line 494, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/asyncio/tasks.py", line 691, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 277, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284838 parent=284705 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284834 parent=284705 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284832 parent=284705 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284780 parent=284705 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284775 parent=284705 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=284769 parent=284705 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/threading.py", line 973, in _bootstrap_inner
    self.run()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/threading.py", line 910, in run
