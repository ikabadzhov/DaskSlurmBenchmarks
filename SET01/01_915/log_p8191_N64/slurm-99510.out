Auks API request failed : krb5 cred : unable to read credential cache
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:32939'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:45925'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:35671'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:41313'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:34075'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:43401'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:40613'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:41639'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:35657'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:44183'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:34315'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:33259'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:37953'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:44559'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:41517'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:32959'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:41277'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:33485'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:43993'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:45017'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:45583'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:45723'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:41311'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:44259'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:36587'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:42623'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:34489'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:42669'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:35891'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:43551'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:44911'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.72:34753'
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-ofz_hrh0', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-g2hp8w8i', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-4ya9dlmb', purging
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:40983
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:40983
distributed.worker - INFO -          dashboard at:           10.32.2.72:38817
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-tpq2eeo6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:44061
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:44061
distributed.worker - INFO -          dashboard at:           10.32.2.72:33499
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-3volwqwr
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:40953
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:40953
distributed.worker - INFO -          dashboard at:           10.32.2.72:39411
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-qkerp3q2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:44675
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:44675
distributed.worker - INFO -          dashboard at:           10.32.2.72:43097
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-bmcwhsbq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:42763
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:42763
distributed.worker - INFO -          dashboard at:           10.32.2.72:34413
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-h2jc6178
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:37873
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:37873
distributed.worker - INFO -          dashboard at:           10.32.2.72:40183
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-gzs1blxm
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:32941
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:32941
distributed.worker - INFO -          dashboard at:           10.32.2.72:44767
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-qb58ate5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:39963
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:39963
distributed.worker - INFO -          dashboard at:           10.32.2.72:46755
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-e94j4oqu
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:36211
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:36211
distributed.worker - INFO -          dashboard at:           10.32.2.72:39913
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-axuvvcak
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:45637
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:45637
distributed.worker - INFO -          dashboard at:           10.32.2.72:43187
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-u866ih4t
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:34809
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:34809
distributed.worker - INFO -          dashboard at:           10.32.2.72:33669
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-ox18t4un
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:41019
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:41019
distributed.worker - INFO -          dashboard at:           10.32.2.72:42991
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-_7e46k_5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:42107
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:42107
distributed.worker - INFO -          dashboard at:           10.32.2.72:41299
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-9f7acqzr
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:41825
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:41825
distributed.worker - INFO -          dashboard at:           10.32.2.72:44251
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-jfofr0n9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:33229
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:33229
distributed.worker - INFO -          dashboard at:           10.32.2.72:38421
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-bug8tt_b
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:45917
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:45917
distributed.worker - INFO -          dashboard at:           10.32.2.72:44705
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-l6gg86nh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:36043
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:36043
distributed.worker - INFO -          dashboard at:           10.32.2.72:39253
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-2vkak0bs
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:39539
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:39539
distributed.worker - INFO -          dashboard at:           10.32.2.72:40623
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-wvbg_lk_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:40685
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:40685
distributed.worker - INFO -          dashboard at:           10.32.2.72:40731
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-rh790jhb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:34731
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:34731
distributed.worker - INFO -          dashboard at:           10.32.2.72:37845
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-a6a8rp92
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:43951
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:43951
distributed.worker - INFO -          dashboard at:           10.32.2.72:32867
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-dm8hnwgh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:38727
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:38727
distributed.worker - INFO -          dashboard at:           10.32.2.72:42241
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-ce_nlsxx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:44759
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:44759
distributed.worker - INFO -          dashboard at:           10.32.2.72:36213
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:41443
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:41443
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -          dashboard at:           10.32.2.72:39383
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-s9euxkds
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-x6i9nbnb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:34065
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:34065
distributed.worker - INFO -          dashboard at:           10.32.2.72:38567
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-2853ahf5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:34545
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:34545
distributed.worker - INFO -          dashboard at:           10.32.2.72:36589
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-0m_f4coa
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:37431
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:37431
distributed.worker - INFO -          dashboard at:           10.32.2.72:42839
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:38453
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-bw8_i80x
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:38453
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:           10.32.2.72:43781
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-ly33hb4f
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:45325
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:45325
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:46539
distributed.worker - INFO -          dashboard at:           10.32.2.72:36687
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-ozgceriz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:46539
distributed.worker - INFO -          dashboard at:           10.32.2.72:38319
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:39641
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.72:33393
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:33393
distributed.worker - INFO -          Listening to:     tcp://10.32.2.72:39641
distributed.worker - INFO -          dashboard at:           10.32.2.72:39131
distributed.worker - INFO -          dashboard at:           10.32.2.72:33509
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-575z0i_g
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-fxcsiipn
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-5ecvb64h
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 198, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py", line 1001, in heartbeat
    response = await retry_operation(
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/utils_comm.py", line 385, in retry_operation
    return await retry(
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 874, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 651, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.32.2.72:37422 remote=tcp://188.185.68.194:46122>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 198, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py", line 1001, in heartbeat
    response = await retry_operation(
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/utils_comm.py", line 385, in retry_operation
    return await retry(
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 874, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 651, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.32.2.72:37424 remote=tcp://188.185.68.194:46122>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/tornado/iostream.py", line 867, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/tornado/iostream.py", line 1140, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py", line 1001, in heartbeat
    response = await retry_operation(
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/utils_comm.py", line 385, in retry_operation
    return await retry(
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 874, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 651, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.32.2.72:37418 remote=tcp://188.185.68.194:46122>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/tornado/iostream.py", line 867, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/tornado/iostream.py", line 1140, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py", line 1001, in heartbeat
    response = await retry_operation(
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/utils_comm.py", line 385, in retry_operation
    return await retry(
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 874, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 651, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.32.2.72:37420 remote=tcp://188.185.68.194:46122>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:40953
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:35671'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:44675
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:41313'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:40983
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:44061
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:32939'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:45925'
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:34489'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:36587'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:35891'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:38453
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:45723'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:34809
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:44911'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:32941
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:45583'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:39641
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:43551'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:33393
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:40685
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:42669'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:45017'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:39539
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:42623'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:34065
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:34753'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:45325
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:44259'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:36211
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:44559'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:43993'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:39963
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:33229
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:33485'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:43951
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:45637
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:44183'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:44759
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:33259'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:36043
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:32959'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:41517'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:37431
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:37953'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:34731
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:41311'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:41825
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:34075'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:34545
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:34315'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:41019
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:41639'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:42763
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:35657'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:46539
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:41277'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:41443
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:40613'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:42107
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:45917
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.72:43401'
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:37873
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.72:38727
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 0 seconds, killing
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/nanny.py", line 335, in start
    response = await self.instantiate()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/nanny.py", line 404, in instantiate
    result = await asyncio.wait_for(
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/asyncio/tasks.py", line 468, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 273, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/asyncio/tasks.py", line 494, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/asyncio/tasks.py", line 691, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 277, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=359746 parent=359633 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=359744 parent=359633 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=359737 parent=359633 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=359722 parent=359633 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=359719 parent=359633 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=359709 parent=359633 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=359701 parent=359633 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=359695 parent=359633 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=359689 parent=359633 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=359683 parent=359633 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/threading.py", line 973, in _bootstrap_inner
    self.run()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/threading.py", line 910, in run
    self._target(*self._args, **self._kwargs)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/process.py", line 234, in _watch_process
    assert exitcode is not None
AssertionError
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/threading.py", line 973, in _bootstrap_inner
