Auks API request failed : krb5 cred : unable to read credential cache
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:41873'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:38483'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:33853'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:37561'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:39985'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:41261'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:44665'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:39521'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:34939'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:44641'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:38685'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:39673'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:34735'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:33355'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:41763'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:39801'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:42195'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:38913'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:45759'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:35647'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:40723'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:36619'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:33863'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:45137'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:39863'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:44273'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:46201'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:37977'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:35151'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:39357'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:42623'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.32.2.11:40583'
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-tvkumtkd', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-7fdntspx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-fgs1viqv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-m0w8b4lx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-awnu8snn', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-l1o51aqy', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-lq5v6ehf', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-gyjvppyp', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-gnkdtw_o', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-hibxd0kr', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-zc0znrd9', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-2cqlkhup', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-or2n_0po', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-blwpqdth', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-d8w_64r9', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-ns0gbeox', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-18_cxmun', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-w3vk16zb', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-ajk2tovj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-ldta98zt', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-3quqqwzx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-zc_cl4im', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-fg0wscn5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-stxdb8os', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-kn5yt_x3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-1teoxkej', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-3j4arrdj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-vcdf48p1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-9uqsnt6h', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-9uda4ch7', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-g4jsf4nf', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/ikabadzh/dask-worker-space/worker-he01irr1', purging
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:40421
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:40421
distributed.worker - INFO -          dashboard at:           10.32.2.11:45915
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-fd0c06jn
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:43977
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:43977
distributed.worker - INFO -          dashboard at:           10.32.2.11:40745
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-nqwidqy7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:45751
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:45751
distributed.worker - INFO -          dashboard at:           10.32.2.11:38713
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-ijl3duw4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:44279
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:44279
distributed.worker - INFO -          dashboard at:           10.32.2.11:36851
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-3tk6xkd0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:42883
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:42883
distributed.worker - INFO -          dashboard at:           10.32.2.11:40145
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-z1styatq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:38103
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:38103
distributed.worker - INFO -          dashboard at:           10.32.2.11:42797
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-e18a0vxi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:43641
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:43641
distributed.worker - INFO -          dashboard at:           10.32.2.11:41445
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-0wr9sycj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:37599
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:37599
distributed.worker - INFO -          dashboard at:           10.32.2.11:35337
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-57xbdbo6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:45707
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:45707
distributed.worker - INFO -          dashboard at:           10.32.2.11:37721
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-ihq2ljy1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:35015
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:35015
distributed.worker - INFO -          dashboard at:           10.32.2.11:35911
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-mak4a4l4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:40103
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:40103
distributed.worker - INFO -          dashboard at:           10.32.2.11:40823
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-v5o9xjpb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:46827
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:46827
distributed.worker - INFO -          dashboard at:           10.32.2.11:34423
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-9og80clg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:45841
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:45841
distributed.worker - INFO -          dashboard at:           10.32.2.11:34797
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-9o0us4n1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:33777
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:33777
distributed.worker - INFO -          dashboard at:           10.32.2.11:37385
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-y3znqeq4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:42563
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:42563
distributed.worker - INFO -          dashboard at:           10.32.2.11:34185
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-pzzddv0y
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:40891
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:40891
distributed.worker - INFO -          dashboard at:           10.32.2.11:46103
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-ozrgkc9w
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:37107
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:37107
distributed.worker - INFO -          dashboard at:           10.32.2.11:41087
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-0cts477v
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:39489
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:39489
distributed.worker - INFO -          dashboard at:           10.32.2.11:44363
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-qe7fi2s8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:33397
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:33397
distributed.worker - INFO -          dashboard at:           10.32.2.11:35611
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-3sra6bvs
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:41323
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:41323
distributed.worker - INFO -          dashboard at:           10.32.2.11:41245
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-wefijbvm
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:34539
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:34539
distributed.worker - INFO -          dashboard at:           10.32.2.11:34623
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-f7kf8fje
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:38317
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:38317
distributed.worker - INFO -          dashboard at:           10.32.2.11:32937
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-4x4aq785
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:35799
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:35799
distributed.worker - INFO -          dashboard at:           10.32.2.11:33067
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-vyupdigc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:42565
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:42565
distributed.worker - INFO -          dashboard at:           10.32.2.11:42841
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-cox9_z05
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:43943
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:43943
distributed.worker - INFO -          dashboard at:           10.32.2.11:37685
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-7c60geci
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:34919
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:34919
distributed.worker - INFO -          dashboard at:           10.32.2.11:34883
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-r17wjqt0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:43863
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:43863
distributed.worker - INFO -          dashboard at:           10.32.2.11:36879
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-8e0fcb3k
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:39987
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:39987
distributed.worker - INFO -          dashboard at:           10.32.2.11:41477
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-scwgteq4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:36351
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:36351
distributed.worker - INFO -          dashboard at:           10.32.2.11:41089
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-clhx_nbn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:38445
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:38445
distributed.worker - INFO -          dashboard at:           10.32.2.11:45523
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-5i2t5ynh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:43443
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:43443
distributed.worker - INFO -          dashboard at:           10.32.2.11:40555
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-h__55fal
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://10.32.2.11:43739
distributed.worker - INFO -          Listening to:     tcp://10.32.2.11:43739
distributed.worker - INFO -          dashboard at:           10.32.2.11:38637
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  14.90 GiB
distributed.worker - INFO -       Local Directory: /tmp/ikabadzh/dask-worker-space/worker-khwey3mj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://188.185.68.194:46122
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 198, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py", line 1001, in heartbeat
    response = await retry_operation(
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/utils_comm.py", line 385, in retry_operation
    return await retry(
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 874, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 651, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.32.2.11:37692 remote=tcp://188.185.68.194:46122>: Stream is closed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 198, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py", line 1001, in heartbeat
    response = await retry_operation(
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/utils_comm.py", line 385, in retry_operation
    return await retry(
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 874, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 651, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.32.2.11:37696 remote=tcp://188.185.68.194:46122>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 198, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py", line 1001, in heartbeat
    response = await retry_operation(
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/utils_comm.py", line 385, in retry_operation
    return await retry(
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 874, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 651, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.32.2.11:37698 remote=tcp://188.185.68.194:46122>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/tornado/iostream.py", line 867, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/tornado/iostream.py", line 1140, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/worker.py", line 1001, in heartbeat
    response = await retry_operation(
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/utils_comm.py", line 385, in retry_operation
    return await retry(
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 874, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 651, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/comm/tcp.py", line 126, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.32.2.11:37700 remote=tcp://188.185.68.194:46122>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:40891
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:37107
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.worker - INFO - Waiting to connect to: tcp://188.185.68.194:46122
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:39985'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:33853'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:37599
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:43641
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:38103
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:42883
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:45751
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:44279
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:45707
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:35015
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:42623'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:34939'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:38483'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:38685'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:42195'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:40583'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:37561'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:46827
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:40103
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:39673'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:38913'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:45841
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:44641'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:33777
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:34735'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:41261'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:38317
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:45137'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:35799
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:35647'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:42565
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:43943
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:45759'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:39357'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:43739
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:36351
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:43443
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:38445
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:39801'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:36619'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:40723'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:41763'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:42563
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:43977
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:40421
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:41873'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:44665'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:46201'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:34539
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:39521'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:37977'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:33355'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:33863'
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:43863
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:34919
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:39987
distributed.worker - INFO - Closed worker has not yet started: Status.undefined
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:39489
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:33397
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://10.32.2.11:41323
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:39863'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:44273'
distributed.nanny - INFO - Closing Nanny at 'tcp://10.32.2.11:35151'
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/nanny.py", line 335, in start
    response = await self.instantiate()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/nanny.py", line 404, in instantiate
    result = await asyncio.wait_for(
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/asyncio/tasks.py", line 468, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 273, in _
    await asyncio.wait_for(self.start(), timeout=timeout)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/asyncio/tasks.py", line 494, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/cli/dask_worker.py", line 469, in <module>
    go()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/cli/dask_worker.py", line 465, in go
    main()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/cli/dask_worker.py", line 451, in main
    loop.run_sync(run)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/tornado/ioloop.py", line 530, in run_sync
    return future_cell[0].result()
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/cli/dask_worker.py", line 445, in run
    await asyncio.gather(*nannies)
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/asyncio/tasks.py", line 691, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/hpcscratch/user/ikabadzh/mambaforge/envs/myenv/lib/python3.9/site-packages/distributed/core.py", line 277, in _
    raise TimeoutError(
asyncio.exceptions.TimeoutError: Nanny failed to start in 60 seconds
